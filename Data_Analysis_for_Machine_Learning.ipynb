{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98728503-5ab82f80-2378-11eb-9c79-adeb308fc647.png\"></img>\n",
    "\n",
    "<h1 style=\"color: white; position: absolute; top:27%; left:10%;\">\n",
    "     INE Bootcamp\n",
    "</h1>\n",
    "<h2 style=\"color: white; position: absolute; top:36%; left:10%;\">\n",
    "    Data Analysis, Visualization and Predictive Modeling\n",
    "</h2> \n",
    "\n",
    "<h3 style=\"color: #ef7d22; font-weight: normal; position: absolute; top:58%; left:10%;\">\n",
    "    <b>David Mertz, Ph.D.</b>\n",
    "</h3>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; font-weight: normal; position: absolute; top:63%; left:10%;\">\n",
    "    <b>Data Scientist</b>\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 200px; background-color: #222; text-align: center; padding-top: 20px; margin-bottom: 40px;\">\n",
    "<br><br>\n",
    "\n",
    "<h1 style=\"color: white; font-weight: bold;\">\n",
    "    Data Analysis for Machine Learning\n",
    "</h1>\n",
    "\n",
    "<br><br> \n",
    "</div>\n",
    "\n",
    "> We have used scikit-learn already in our basic polynomial fitting.  but let us use it in a more systematic way, and consider some issues that we need to in real-world data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with a Kaggle dataset: [House Sales in King County, USA](https://www.kaggle.com/harlfoxem/housesalesprediction).\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/110563684-6a7b3100-812a-11eb-961b-ec7d2f25c008.jpg\" style=\"width:400px; float: right; margin: 0 40px 40px 40px;\"/>\n",
    "\n",
    "These are the features of the dataset:\n",
    "\n",
    "* **id**: a notation for a house\n",
    "* **date**: Date house was sold\n",
    "* **price**: Price is prediction target\n",
    "* **bedrooms**: Number of Bedrooms/House\n",
    "* **bathrooms**: Number of bathrooms/bedrooms\n",
    "* **sqft_living**: square footage of the home\n",
    "* **sqft_lot**: square footage of the lot\n",
    "* **floors**: Total floors (levels) in house\n",
    "* **waterfront**: House which has a view to a waterfront\n",
    "* **view**: Has been viewed\n",
    "* **condition**: How good the condition is ( Overall )\n",
    "* **grade**: overall grade given to the housing unit, based on King County grading system\n",
    "* **sqft_above**: square footage of house apart from basement\n",
    "* **sqft_basement**: square footage of the basement\n",
    "* **yr_built**: Built Year\n",
    "* **yr_renovated**: Year when house was renovated\n",
    "* **zipcode**: zip\n",
    "* **lat**: Latitude coordinate\n",
    "* **long**: Longitude coordinate\n",
    "* **sqft_living15**: Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area\n",
    "* **sqft_lot15**: lotSize area in 2015(implies-- some renovations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Exploratory Data Analysis\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/kc_house_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Getting a feeling for the data\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "The first step when analyzing data is cleaning. Understanding if we've loaded the data correctly and we have valid values. This is a process that will involve multiple steps, but for now, we start with our _5 minute_ check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `shape` we know that there are 21,613 rows, with 21 columns (features). Let's check for red flags on those features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`info` gives you a quick summary of both the type and the count for each column. In this case the data seems correct, there are no missing values and the types are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip code is interesting.  House prices are often driven by zip code (\"desirable\" neighborhoods), but the numeric order of these codes has no pattern in relation to these prices.  The zip code is known as a \"categorical variable\" rather than a quantitative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.zipcode.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date is now encoded as a timestamp string, which is not directly useful.  We can convert it to a datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.date[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.to_datetime(df.date)\n",
    "days.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, that form is also not yet directly useful since machine learning models want *numbers* to work with.  We can continue conversion to get this into \"nanoseconds since the epoch\" (the beginning of Unix time in 1970).  The specific numbers are not important, but only that they go up with the passage of time.  Let us add that as a potentially useful feature.  I.e. perhaps housing prices change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['since_epoch'] = days.astype(int)\n",
    "df.iloc[:5, -5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    High-level feature selection\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Our objective is to predict the price of a house based on the features that we know about the house. For example, we know that a larger lot size and more bedrooms will relate with a highest price. It makes sense to drop the internal ID, and the unencoded date. Let us drop zip code for the moment, but we will come back to it. Latitude and longitude measure something similar about geographic location, and we retain them.\n",
    "\n",
    "Feature selection can be very important to an ML model. With pandas is simple to exclude columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes = df.zipcode  # Save for later\n",
    "df.drop(columns=['id', 'zipcode', 'date'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Correlation between variables\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Some variables will have higher (positive or negative) correlation with the price. We know that the surface area of a house is positively correlated with its price: the larger the house, a higher price. But what about others? We can build a simple correlation plot to understand a little bit better the relationship between different variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['price', 'since_epoch', 'bedrooms', 'bathrooms', 'sqft_above', 'sqft_living', 'lat', 'long']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit interesting that latitude is somewhat correlated with price, and longitude essentially not at all. However, we probably expect that it is the *interaction* of these variables that measure an actual effect.  That is, the \"rich neighborhood\" is not necessarily the one farthest north, south, east, or west; quite likely it is some region in the middle, along both axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a visualization to summarize these variables and their correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "sns.heatmap(corr, ax=ax, linewidths=0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us think about the fairly strong correlation of `grade` and `price`?  Is it a linear relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.corr().loc['grade', 'price'])\n",
    "df.plot.scatter(x='grade', y='price', figsize=(15, 4), color=\"darkblue\", marker='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It feels like there is some connection between the ordinal grade and the quantitative price.  But we have quite a bit of variability of price within a grade.  Looking at price logarithmically makes the pattern a bit sharper, but still not entirely so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='grade', y='price', figsize=(15, 4), \n",
    "                logy=True, color=\"darkblue\", marker='.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    More cleaning, identifying outliers\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Linear regression (along with other ML models) can be sensitive to outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔 A house with 33 bedrooms? There's something going on here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "sns.boxplot(data=df[['bedrooms', 'bathrooms']], orient=\"h\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense for a (really expensive) house to have, let's say 10 bedrooms, but 33 seems like an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['bedrooms'] == 33].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33 bedrooms and only 1.75 bathrooms? 😅 clearly an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(15870, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what about those properties without bathrooms? That is strange, let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['bathrooms'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we look, it perhaps makes a little bit more sense. Maybe those are just warehouses or other type of storage unit facilities? Without more information is now difficult to make a decision. This is an important lesson: **domain expertise is fundamental when analyzing data**\n",
    "\n",
    "We will not remove any additional house at this point, just keep a mental note of the suspicious absence of bathrooms listed. How are other variables doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "sns.boxplot(data=df[['sqft_living','sqft_above', 'sqft_basement']], orient=\"h\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probably requires a little bit more analysis, but let's proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    One-hot encoding\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "The `zipcode` feature we dropped above raises a problem. Machine learning models often do not understand categorical features like zip code. To a machine learning algorithm, a `zipcode` value of 98199 is \"greater than\" 98102, which is greater than 98001. However, the more expensive houses are unlikely to follow this same order (nor the exact reverse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data, there are 70 zip codes, and varying numbers of houses in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create \"dummies\" to represent these categorical values as new numeric variables.  This is what is called \"one-hot\" encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We *could* add these 70 new features to our dataset, which would then be entirely numeric.  We do not do so at this point, however (not out of principle, just convenience of pedagogy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Feature scaling and normalization\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "There is a final **IMPORTANT** point to discuss: \"scaling\" and \"normalizing\" features. It has a mathematical explanation, but basically, what we **do not** want is to have features whose units occupy dramatically different numeric ranges. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['bedrooms', 'sqft_lot', 'since_epoch']\n",
    "df[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values here are too dissimilar, which will make many algorithms perform poorly. We can scale these features to remove the units. \n",
    "\n",
    "Read more here: [Importance of Feature Scaling](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = StandardScaler().fit_transform(df[cols])\n",
    "df_scale = pd.DataFrame(scaled, columns=['scaled_br', 'scaled_lot', 'scaled_epoch'])\n",
    "df_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "sns.violinplot(data=df_scale, orient='h');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some lot sizes that are far larger than is typical, but at least we do not have 18 orders of magnitude difference between the kinds of variables.  Most likely, this is accurate data, but includes some large farms or ranches along with ¼ acre city lots. All have a mean of zero and standard deviation of one in their scaled version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale all of our data and separate features from target. The capital \"X\" for the (multiple) features—i.e. independent variables—and the lower \"y\" for the (single) target are a very common convention harkening back to high school algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price', axis=1)   # everything except the price\n",
    "y = df.price                   # just the price\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Train/test splits\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "There is another **very important** topic we skipped in the look and linear and polynomial fitting.  We earlier used *ALL* the data for both training and prediction (or scoring).  This leads to a problem called *overfitting*.  The model learns to memorize the data it is given rather than genuinely model the underlying behavior.\n",
    "\n",
    "The way we deal with this problem is to split the data into two parts, one to perform the training with, the second to hold in reserve for evaluation of the quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=1)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test.shape: \", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold;\">\n",
    "    Modeling\n",
    "</h2>\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Let's see now how our Linear Regression performs on our cleaned and scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=1)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"Score: {score:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It **seems** a little better than in the initial lesson, as a straight linear regression.  However, in reality, it is actually quite a **lot** better because this is a fair model that uses a train/test split.  Doing the split will always reduce the score, but the higher score of the unsplit data is purely overfitting, and will fail dramatically when it sees novel data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try also with a polynomial, as we did in the earlier lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = PolynomialFeatures(2).fit_transform(X_scaled)\n",
    "X_polytrain, X_polytest, y_train, y_test = train_test_split(X_poly, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_polytrain, y_train)\n",
    "score = model.score(X_polytest, y_test)\n",
    "print(f\"Score: {score:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_polytest[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models that are not linear, nor even polynomial, are also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "model = KNeighborsRegressor().fit(X_polytrain, y_train)\n",
    "score = model.score(X_polytest, y_test)\n",
    "print(f\"Score: {score:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "model = HistGradientBoostingRegressor().fit(X_polytrain, y_train)\n",
    "score = model.score(X_polytest, y_test)\n",
    "print(f\"Score: {score:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 200px; background-color: #ef7d22; text-align: center; padding-top: 20px; margin-bottom: 40px;\">\n",
    "<br><br>\n",
    "\n",
    "<h1 style=\"color: white; font-weight: bold;\">\n",
    "    Exercises\n",
    "</h1>\n",
    "\n",
    "<br><br> \n",
    "</div>\n",
    "\n",
    "Within scikit-learn itself, a number of datasets are made available.  One often used one contains similar house sales data as that we looked at here.  These datasets are not stored as Pandas DataFrames, but we can easily construct one from the attributes of a dataset object.  First take a look at the attributes provided using `dir(ca_housing)` and try to understand what each is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "ca_housing = datasets.fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a DataFrame, then perform the same cleanup and modeling as we did for the King County data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = pd.DataFrame(ca_housing.data, columns=ca_housing.feature_names)\n",
    "df_ca['TARGET'] = ca_housing.target\n",
    "df_ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and evaluate a model to predict (1990s) California housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 400px; background-color: #222; text-align: center; padding-top: 120px;\">\n",
    "<br><br>\n",
    "\n",
    "<h1 style=\"color: white; font-weight: bold;\">\n",
    "    Review and questions\n",
    "</h1>\n",
    "\n",
    "<br><br> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 400px; background-color: #ef7d22; text-align: center; padding-top: 120px;\">\n",
    "<br><br>\n",
    "\n",
    "<h1 style=\"color: white; font-weight: bold;\">\n",
    "    <a style=\"color: white;\" href=\"https://docs.google.com/forms/d/1FGx7gzZzOgahGF1X6ZOOo2nGMHbHpHIqMysdYg5_WBw/viewform?edit_requested=true\" target=\"_blank\">Evaluation</a>\n",
    "</h1>\n",
    "\n",
    "<br><br> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98864025-08deda80-2448-11eb-9600-22aa17884cdf.png\" style=\"height: 100%; max-height: inherit; position: absolute; top: 20%; left: 0px;\"></img>\n",
    "<br>\n",
    "\n",
    "<h2 style=\"font-weight: bold;\">\n",
    "    David Mertz, Ph.D.\n",
    "</h2>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; margin-top: 0.8em\">\n",
    "    Data Scientist\n",
    "</h3>\n",
    "<hr>\n",
    "<br><br>\n",
    "\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    david.mertz@gmail.com\n",
    "</p>\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    linkedin.com/in/dmertz/\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
